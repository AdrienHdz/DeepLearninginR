\documentclass[11pt,letter]{article} 

%%%%%%%%%%%%%%%%%%%%
%%%%%% Package %%%%%
%%%%%%%%%%%%%%%%%%%%

%\usepackage[latin1]{inputenc}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{amsmath,amsthm,amssymb,bbm} %math stuff
\usepackage{placeins} % FloatBarrier
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{float}    % for fig.pos='H'
\usepackage{rotfloat} % for sidewaysfigure
%\usepackage{subfig}   % for subfigure
\usepackage{subcaption}  % an alternative package for sub figures
\usepackage{comment}
\bibliographystyle{plainnat}
\usepackage{setspace} %Spacing
\usepackage{graphicx,graphics}
\usepackage{booktabs,tabularx}
\usepackage{enumerate}
\usepackage{makecell}
\usepackage{xfrac}
\restylefloat{figure}
\usepackage{appendix}
\usepackage{color, colortbl, xcolor}
\usepackage{booktabs,dcolumn} % for use with texreg in R
\usepackage[bookmarks]{hyperref}
\usepackage[backend=bibtex]{biblatex}
\usepackage{wrapfig}
\usepackage{todonotes}
\usepackage{ctable}
\usepackage{acronym}
\usepackage{fancyvrb}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Configuration %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{global}
<<biber, eval=TRUE, include= FALSE, cache=FALSE, echo= FALSE >>=
library('knitr')
system (paste("biber", sub("\\.Rnw$","",current_input())))
@
\newcommand{\nd}{\noindent}
\newcommand{\ntodo}[2][]{\todo[#1]{\thesubsubsection{}. #2}}

% Define the format of the report
\renewcommand{\headrulewidth}{0.0pt}
\renewcommand{\footrulewidth}{0.0pt}
\setlength{\textheight}{9.00in}
\setlength{\textwidth}{7.00in}
\setlength{\topmargin}{-0.5in}
\setlength{\evensidemargin}{-0.25in}
\setlength{\oddsidemargin}{-0.25in}
\renewcommand{\baselinestretch}{1.2}
\makeatletter
\makeatother
\lfoot{} \cfoot{ } \rfoot{{\small{\em Page \thepage \ of \pageref{LastPage}}}}

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% BEGIN        %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\pagestyle{plain}
\pagenumbering{gobble}

\title{%
 Méthodes avancées en exploitation de donnée \\
  \large (MATH80619)}
\author{\begin{tabular}{ll}
    Estefan Apablaza-Arancibia & 11271806\\
        Adrien Hernandez & 11269225\\

    
\end{tabular}}
\maketitle
\newpage
\pagestyle{plain}
\section*{List of Acronyms}
\begin{acronym}
\acro{API}{Application Programming Interface} 
\acro{CNN}{Convulutional Neural Network} 
\acro{FNN}{Feed-Forward Neural Network} 
\acro{RNN}{Recurent Neural Network} 

\end{acronym} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\section{Introduction}
In the first section of this paper, a literature review covers the neural networks and deep learners algorithms, focusing on different type of neural networks architecture; the purpose of adding multiple hidden layers and, ultimately, what are the challenges regarding the increase in computing time. Furthermore, in the methodology section, a list of deep learning projects are shown in order to understand some patterns and methods. Then, an exhaustive list of the R libraries allowing to build neural networks and deep learners models. Correspondingly, the advantages and disadvantages of each libraries, their capabilities as well as what you can expect when using them. To sum up, the last section will give concrete examples on how to implement the neural networks and deep learners models with these libraries, using real data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Litterature %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Review      %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Litterature Review}
\subsection{What is Deep Learning and why use it?}
\begin{wrapfigure}[14]{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{deep learning.PNG}
      \end{center}
     \caption{Artificial Intelligence vs Machine Learning vs Deep Learning}
     \label{fig:simule}
\end{wrapfigure}

Deep learning is a subset field of articial intelligence and can be seen as a specific way of doing machine learning. Deep learning algorithms can be seen as feature representation learning. Indeed, by applying to the data multiple non-linear transformations through the use of hidden layers, deep learning models have their own trainable feature extration capability making it possible to learn specific features from the data without needing a specific human domain expert. This means that deep learning models won't require the features extraction step that is essential for classic machine learning models. However, increasing the models capacity by adding hidden layers, requires increasingly computing power and slow down the training process of the model. The choice of hyperparameters, programming languages and memory management will therefore be important criteria to take into account while building deep learning models\\
Since the last decade, deep learning models have shown notable predictive power and have been revolutionizing an important number of domain such as computer vision, natural language understanding, fraud detection, health and much more.\\
As a first glance in the subject, it is highly recommended it to read  a reference from pioneers in the field (\cite[Chapter 1]{Goodfellow-et-al-2016}).


\subsubsection{Feed Forward Neural Network}
\begin{wrapfigure}[12]{R}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{feedforward_neural_networks.png}
  \end{center}
  \caption{feedforward neural networks}
  \label{fig:attention}
\end{wrapfigure}

"Deep feedforward networks, also called feedforward neural networks, or multilayer perceptrons (MLPs), are the quintessential deep learning models." \cite{Goodfellow-et-al-2016}

\ac{FNN} models are inspired from biology and by the way the human brain works. In a neural network, each neuron takes input from other neurons, processes the information and then transmits outputs to next neurons. Artificial neural networks follow the same process as each neuron will perform the weighted sum of inputs and will add a bias as a degree of freedom. It will then apply a non-linear transformation before outputting the information. Thus, the information goes forward in the network; neurons transmit information from the input layers towards the output layer. It is important to know that in a feedfoward neural networks (fig. \ref{fig:attention}) the neurons of a same layer are not connected to each other; they do not allow any feedback connections within a same layer. If we want to allow this process, we will be looking at recurrent neural networks.

The equation a neuron input is  
\begin{equation}
a(x) = b +\sum_{i}{w_i x_i}
\end{equation}
and the output
\begin{equation}
h(x) = g(a(x))
\end{equation}
where:
\begin{conditions}
 x     &  the input data \\
 b     &  the bias term \\
 w     &  the weight or parameter \\   
 g(...) &  the activation function
\end{conditions}

Here, \textbf{the bias term  b} and \textbf{the weights or parameter w} will be learned by the model in order the minimize a cost function, through the use of the gradient descent method. Then, the network will use the backpropagation algorithm where the error is backpropagated from the output to the input layers and the bias and weighted are then updated accordingly.\\
Regarding the \textbf{activation function g(...)}, the common practice is to use a ReLu (rectified linear unit) as the activation function for the neurons of the hidden layers. Regarding the neurons of the last layer, the activation function will be chosen accordingly to the task we want our model to perform:\\
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{activation_function_output.png}
     \caption{Activation functions: output units. HEC}
     \label{fig:simule}
\end{figure}

A detailed explanation of the theory of feedforward neural network can be found in \cite[Chapter 6]{Goodfellow-et-al-2016}

\subsubsection{\ac{CNN}}

The \ac{CNN} are a modified architecture of \ac{FNN} that leverage the feature engineering that used to be hand maded by domain experts. This class of deep neural network are commonly use for image recognition and classification that can serve different applications such as facial recongnition, document analysis and speech recognition. The original \ac{FNN} are not suited analyzing large size images since the weights increase exponentially and, at the end, don't perfom very well.\\
\begin{wrapfigure}[12]{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{CNN_process.png}
  \end{center}
  \caption{CNN, Deep Learning A-Z by SuperDataScience}
  \label{fig:attention}
\end{wrapfigure}
A standard architecture of a CNN model is commonly represented this way: We start with an input image to which we are going to apply several kernels, a.k.a features detectors, in order to create feature maps that will form what we call a convolutional layer. A common practice is to apply the ReLu activation fonction to the convolutional layer output in order to increase the non linearity of the images before using the pooling method to create a pooling layer. The next step will be to combine all the pooled images from the pooling layer into one single vector, which is called flattening and will be the input of a FNN that will perform the desired task, for instance, classification. CNN parameters are trained with the backpropagation algorithm where the gradients are propagated from the output layer of the FFN to the input of the CNN in order to minimise a cost function, most of the time a categorical cross-entropy if we are performing a multi-class classification. 


To get a detailed explanation of convolutional neural networks we recommand to read \cite[Chapter 9]{Goodfellow-et-al-2016}.\\

\subsubsection{RNN}
Recurrent neural networks are a more advanced type of neural networks architecture having proven state-of-art performance for solving tasks related to sequential data such as Natural Language Processing (NLP), anomaly detection and event forecasting. As a key differentiator from feedfoward neural networks, is that RNNs use feedback loops connections instead of feedforward connections and get an internal memory. Indeed, that take as input each step of the sequantial data as well as what they have learned over time. One of their other advantage is to be able to handle input and ouput sequences with different sizes. "A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor" Olah.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{RNN_process.png}
     \caption{RNN, Understanding LSTM Networks by Olah}
     \label{fig:simule}
\end{figure}

To get a detailed explanation of the theory of recurrent neural networks we recommand to read \cite[Chapter 10]{Goodfellow-et-al-2016}. We also recommand reading this blog post \url{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}

\subsection{How to integrate Deep Learning in R}
The integration of Deep learning in R can be separate in two part ; (1) The \ac{API} integration and (2) the standalone R packages. 
\begin{enumerate}
\item The \ac{API} will give the possibility to control externally through R an existing installation. In other words, a software translator for a already known software installation. This approach is not always trivial to install nor to manipulate but in long term will probably give the best flexibility in terms of Deep Learning projects.
\item The standalone R packages will be packages that require no third party software in order to create the deep learning projects. This approach is relatively fast to install but is restraing in terms deep learning architecture.
\end{enumerate}
A list of the available resources with installation and examples tutorials can be found in section \ref{avail_ressour} and \ref{examples} .
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Description %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% of methods  %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Description of methods}
\ntodo[inline]{I think this section is more to explain, for example, the dataset with their end objective so when we are doing examples we just say we are going to use the CIFAR10 and the readers know we are doing a classification problem.}

\section{Methodology}
\subsection{Dataset}
\subsubsection{FNN}
\subsubsection{CNN}
\subsubsection{RNN}
\subsection{Benchmark}
\subsubsection{Accuracy}
\subsubsection{Time elapsed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Resources   %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% available   %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Available resources}
\label{sec:avail_ressour}
\ntodo[inline]{Très intéressant:\url{https://edu.kpfu.ru/pluginfile.php/419285/mod_resource/content/2/neural-networks-r.pdf}}
According to the CRAN-R project website, we give a list of the different packages known in R allowing to use simple and deep neural network algorithms. \newline \url{https://cran.r-project.org/web/views/MachineLearning.html}

\subsection{R packages}
\url{http://www.parallelr.com/r-deep-neural-network-from-scratch/}
\url{https://edu.kpfu.ru/pluginfile.php/419285/mod_resource/content/2/neural-networks-r.pdf}
\subsubsection{nnet package}
\textbf{Keyword: FNN}
\paragraph{Description}
This package comes from the CRAN platform and allows to build and fit "FNN with a single hidden layer and multinomial log-linear models". This package uses C/C++ in backend.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{nnet_package.png}
     \caption{Package nnet, CRAN}
\end{figure}
\paragraph{Pros}
\begin{itemize}
\item One of the easiest R package to build a quick FFN model.
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item It does not offer a lot of flexibility, and can only apply logistic sifmoid function for the hidden layer activation and cannot use tanH and ReLu.
\item This package does not allow to use more that one hidden layer, and does not have any feature to find the optimal number of neurones in the hidden layer. It is up to the analyst to build a loop to test by cross-validation, for exemple, the optimal hyperparameter values.
\item Cannot use classical backpropagation algorithm to train the network. It only uses the BFGS (Broyden-Fletcher-Goldfarb-Shanno) which is a Quasi-Newton method and therefore increases the number of computation to reach a local optima. However, applied on a small dataset with a model having only one hidden layer does not seem to be a problem.
\item Does not have any function to plot the model.
\end{itemize}

\subsubsection{NeuralNetTools package}
\textbf{Keywords: Visualization; Variable importance; Sensitivity}
\paragraph{Description}
This package is a complement to R's neural networks packages as it allows to visualize and perform analysis on neural network models. "Functions are available for plooting, quantifying variable importance, conducting a sensitivity analysis, and obtaining a simple list of model weights."
\begin{figure}[H]
  \begin{subfigure}{0.5\textwidth}
    \centering\includegraphics[width=0.45\textwidth]{plotnet.png}
    \caption{NeuralNetTools, plotnet function}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering\includegraphics[width=0.45\textwidth]{garson_algorithm.png}
    \caption{NeuralNetToolsm Input variables importance}
  \end{subfigure}
    \caption{Embedding dimension hyper parameter}
\end{figure}

\paragraph{Pros}
\begin{itemize}
\item Very easy to plot your neural network models with the function \textbf{plotnet(model)}.
\item Visualize the input variables importance to the output prediction witht the Garson and olden algorithm.
\item Perform sensitivity analysis on your neural networks model using the Lek profile method.
\item Works with the models built from different packages: \textbf{caret, neuralnet, nnet, RSNSS}
\item Can plot neural networks with pruned connections from the RSNSS package.
\end{itemize}

\paragraph{Cons}
\begin{itemize}
\item Does not provide any function for neural network model development.
\item Is not optimized to visualize large neural networks models.
\end{itemize}
\subsubsection{Neuralnet package}
\textbf{Keyword: FNN; Deep FNN}
\paragraph{Description}
According to its CRAN description, the package allows the "training of neural networks using the backpropagation, resilient backpropagation with (Riedmiller, 1994) or without weight backtracking (Riedmiller, 1993) or the modified globally convergent version by Anastasiadis et al. (2005). The package allows flexible settings through custom-choice of error and activation function. Furthermore, the calculation of generalized weights (Intrator O \& Intrator N, 1993) is implemented." This package uses C$\$/C\+\+ in backend.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{neuralnet_package.png}
     \caption{Package neuralnet, CRAN}
\end{figure}

\paragraph{Pros}
\begin{itemize}
\item Very easy to use and to build a quick FFN and deep FNN model.
\item Allows to use several types of backpropagation algorithms. By default, the resilient backpropagation algorithm is used "rprop+" instead of regular backpropagation which is an algorithm not very used in practice as it is more tricky to implement. We can however decided to change the rprop+ for a standard backpropagation by changing for \textbf{algorithm = "backprop"}.
\item Several hidden layers and neurons per layer can be added. By default, the algorithm uses only 1 hidden layer with 1 neuron, but it can be increased by addind the command line: \textbf{hidden = c(5,2)} to get 2 hidden layers of 5 neurons.
\item Allows to easily plot and visualize the model and its parameters with the command line: \textbf{plot(model).}
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item One of the disadvantage of this package is that it requires some data preprocessing as it only works with numeric inputs. Therefore, factor variables will need to be transformed into dummies during the preprocessing phase. 
\item The package doesn't provide built-in normalization functions. Therefore, it is recommended to manually normalize the data before using them as input in the neural networks in order to reduce the number of iretation of the algorithm.
\end{itemize}

\subsubsection{Deepnet package}
\textbf{Keywords: FNN; RBM; DBN; Autoencoder}
\paragraph{Description}
This package is available on the CRAN platform and has been written specifcally written for R. It allows "implement som deep learning architectures and neural network algorithms, including backpropagation, Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN) and Deep autoencoder".
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{deepnet_package.png}
     \caption{Package deepnet, CRAN}
\end{figure}
\paragraph{Pros}
\begin{itemize}
\item Allows to load benchmark datasets such as MNIST with the function load.mnist(dir)
\item Allows to initialize the weights of a FNN with the Deep Belief Network algorithm (dbn.dnn.train()) or Stacked AutoEncoder algorithm (sae.dnn.train()).
\item Allows to have an estimation of the probabilistic distribution of a dataset trough the use of the Restricted Boltzmann machine algorithm.
\item Have more activation functions than the other R packages. Hidden Layers activation function can be linear, sigmoid or tanh. The output activation function can be linar, sigmoid or softmax.
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item Does not support the ReLu activation function for hidden layers.
\item Not the fastest package due to its implementation in R.
\item Does not provide a lot of hyperparameters tuning options, and is not user-friendly.
\end{itemize}

\subsubsection{brnn package}
\textbf{Keywords: Bayesian Regularization; FNN}
\paragraph{Description}
This package available on the platform CRAN, allows to perform "Baeysian regularized neural networks inclusing both additive and dominance effects, and allows to takes advantage of multicore architectures via a parallel computing approach using openMP for the computations". (Pérez-Rodriguez, ...)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{nnet_package.png}
     \caption{Package nnet, CRAN}
\end{figure}
\paragraph{Pros}
\begin{itemize}
\item Allows to add Additive and Dominance effects in the neural networks models.
\item Take advantage of multicore processors (only for UNIX-like systems).
\item Allows to use a Gauss-Newton algorithm for optimization.
\item The package is able to assign initial weights by using the Nguyen and Widrow algorithm.
\item Include an algorithm to deal with ordinal data by using the function \textbf{brnn\_ordinal(x, ...)}
\item It has a function to normalize and unnormalized the data.
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item Cannot use the classic backpropagation algorithm for optimization.
\item The package fits a two layers neural networks and it is not possible to increase the number of hidden layers.
\end{itemize}

\subsubsection{rnn package}
\textbf{Keywords: RNN}
\textbr{Installation:} You will need to install the "diges" package first through the function \textbf{install.packages("digest")} in order to download the rnn package.
\paragraph{Description}
Avaible from the CRAN platform, this package allows the "implementation of a Recurrent Neural Network architectures in native R, including Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and vanilla RNN. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{rnn_package.png}
     \caption{Package rnn, CRAN}
\end{figure}
\paragraph{Pros}
\begin{itemize}
\item Allows to run a demonstration of how RNN models work by using predefined values and allowing the user to see the impact of a changes in the model's hyperparameters by using the function \textbr{run.rnn\_demo()}
\item Allows to plot the model's errors through all epochs.
\item Allows to use LSTM and GRU models.
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item Uses R native, therefore it might not be the best package in terms of time computation.
\end{itemize}

\ntodo[inline]{Bonne ressource pour videos \url{https://www.youtube.com/user/westlandindia/videos}}

\subsection{\ac{API} packages}
\subsubsection{Keras}
\paragraph{Description}

\paragraph{How to install it}
\ntodo[inline]{Très très intéressant:\url{https://r2018-rennes.sciencesconf.org/data/pages/Deep_with_R_1.pdf}}
First step, is the installation and download of the Keras files from GitHub :
<<eval=FALSE, echo=TRUE, highlight=TRUE>>=
install.packages("devtools")
devtools::install_github("rstudio/keras")
@
Then, we need to to install the package and import in the project :
<<echo=TRUE, message=FALSE>>=
library(keras)
install_keras()
@
When the "\textit{Installation complete.}" message appear you have a complete installation with CPU configure on the TensorFlow backend.
If you want to take advantage of your GPU (Ensure that you have the hardware prerequisites) you will need to execute a different command as follows:
<<eval=FALSE, echo=TRUE, highlight=TRUE>>=
install_keras(tensorflow = "gpu")
@

\subparagraph{Tensorflow backend}
\subparagraph{CNTK backend}
\subparagraph{Theano backend}
\paragraph{Pros}
\paragraph{Cons}
%$https://keras.rstudio.com/$
\subsubsection{Tensorflow}
\paragraph{Description}
\paragraph{Pros}
\paragraph{Cons}

\url{https://srdas.github.io/DLBook/DeepLearningWithR.html#using-tensorflow}

\subsubsection{MXNET}
\paragraph{Description}
\paragraph{Pros}
\begin{itemize}
\item Allows the use of multiple CPU and multiple GPU, but it requires to download the package Rtools and a c++ compiler.
\item Very easy to use and has a flexible implementation of diffenent neural networks architectures and models.
\item The models can be built layer per layer.
\item Provide details and information about the learning progress during the training phase.
\end{itemize}
\paragraph{Cons}

\url{https://srdas.github.io/DLBook/DeepLearningWithR.html#using-mxnet}

\subsubsection{rTorch}

\subsubsection{H2o} 
\paragraph{Description}
H2o is a "scalable open source machine learning platform that offers parallelized implementations of many supervised, unsupervised and fully automatic machine learning algorithms on clusters". This package allows to run H2o via its REST API through R and offers several advantages such as the ability ot know the computation time remaining when running a model.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{h2o_package.png}
     \caption{H2o package}
\end{figure}
\paragraph{How to install it}
<<eval=FALSE, echo=TRUE, highlight=TRUE>>=
install.packages("h2o")
library(h2o)
@
\textbf{Prerequisites to launch H2o}, 64 bit Java 6+ if you want to open a h2o model that s more than 1 GB. 
\paragraph{Pros}
\begin{itemize}
\item Very easy to use and includes a cross-validation feature and functions for grid search in order to optimize the model's hyperparameters.
\item 
\item Allows the use of multiple CPU.
\item Provide an adaptive learning rate (ADADELTA) which improves the optimization process by having a different learning rate for each neuron.
\item Provide details and information about the learning progress during the training phase.
\end{itemize}
\paragraph{Cons}
\begin{itemize}
\item Requires the latest version of Java.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Examples    %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples}
\label{sec:examples}
Parler des packages quon décider de tester et comparer.
\subsection{Installation}
Cette section est surtout pour les resources API, car les ressources R sont tout simplement
\subsubsection{Keras}

\subsection{\ac{FNN}}
\subsubsection{Ressources}
\VerbatimInput{fnn.r}

\subsubsection{Compare}

\subsection{\ac{CNN}}
\subsubsection{Ressources}
\subsubsection{Compare}

\subsection{\ac{RNN}}
\subsubsection{Ressources}
\subsubsection{Compare}


\FloatBarrier 
\clearpage 
\pagestyle{plain}
\ntodo[inline]{Find a way to remove page number from references}

\printbibliography



\end{document}
